{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TIME GAN "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Idea & Challenge & Explanation\n",
    "- A generative model for time-series data should also learn the temporal dynamics that shapes how one sequence of observations follows another \n",
    "- The model learns a time-series embedding space while optimizing both supervised and adversarial objectives that encourage it to adhere to the dynamics observed while sampling from historical data during training.\n",
    "- A successful generative model for time-series data needs to capture both the cross-sectional distribution of features at each point in time and the longitudinal relationships among these features over time. Expressed in the image context we just discussed, the model needs to learn not only what a realistic image looks like, but also how one image evolves from the next as in a video.\n",
    "\n",
    "#### ! What differentiate it from \"RNN\" Gan \n",
    "TimeGAN explicitly incorporates the autoregressive nature of time series by combining the unsupervised adversarial loss on both real and synthetic sequences familiar from the DCGAN example with a stepwise supervised loss with respect to the original data. The goal is to reward the model for learning the distribution over transitions from one point in time to the next present in the historical data\n",
    "\n",
    "## Ressources\n",
    "- [Kaggle Notebook](https://www.kaggle.com/code/faaizhashmi/generating-synthetic-data-apple-stock-using-gan) \n",
    "- [Medium article ](https://towardsdatascience.com/synthetic-time-series-data-a-gan-approach-869a984f2239)\n",
    "- [Paper](https://papers.nips.cc/paper/2019/file/c9efe5f26cd17ba6216bbe2a7d26d490-Paper.pdf)\n",
    "- [Generative Adversarial Nets for Synthetic Time Series Data](https://github.com/stefan-jansen/machine-learning-for-trading/blob/main/21_gans_for_synthetic_time_series/README.md)\n",
    "\n",
    "## Architecture \n",
    "- TimeGAN architecture introduces the concept of supervised loss —the model is encouraged to capture time conditional distribution within the data by using the original data as a supervision. Also, we can observe the introduction of an embedding network that is responsible to reduce the adversarial learning space dimensionality.\n",
    "\n",
    "- As mentioned above, TimeGAN is a framework to synthesize sequential data compose by 4 networks, that play distinct roles in the process of modelling the data: the expected generator and discriminator, but also, by a recovery and embedder models.\n",
    "\n",
    "### Components (Class) you need to define \n",
    "\n",
    "[See this ](https://github.com/stefan-jansen/machine-learning-for-trading/blob/main/21_gans_for_synthetic_time_series/README.md#the-four-components-of-the-timegan-architecture)\n",
    "1. Supervisor : Take care of the latent space\n",
    "2. Generator (Adversarial Network): \n",
    "3. Discriminator (Adversarial Network): \n",
    "4. Recovery (AE): \n",
    "5. Embedder (AE): \n",
    "\n",
    "The key insight is that the autoencoding components are trained jointly with the adversarial components, such that TimeGAN simultaneously learns to encode features, generate representations, and iterate across time. The embedding network provides the latent space, the adversarial network operates within this space, and the latent dynamics of both real and synthetic data are synchronized through a supervised loss.\n",
    "\n",
    "\n",
    "### Loss function\n",
    "1. The reconstruction loss, which refers to the auto-encoder (embedder & recovery), that in a nutshell compares how well was the reconstruction of the encoded data when compared to the original one.\n",
    "2. The supervised loss that, in a nutshell, is responsible to capture how well the generator approximates the next time step in the latent space.\n",
    "3. The unsupervised loss, this one it’s already familiar to us, a it reflects the relation between the generator and discriminator networks (min-max game). \n",
    "\n",
    "### Training phases\n",
    "1. Training the autoencoder on the provided sequential data for optimal reconstruction\n",
    "2. Training the supervisor using the real sequence data to capture the temporal behavior of the historical information, and finally,\n",
    "3. The combined training of four components while minimizing all the three loss functions mentioned previously.\n",
    "\n",
    "### Concept \n",
    "- Embedding network \n",
    "- Time conditional distribution \n",
    "- adversarial learning space\n",
    "- [Stepwise dependency (and stepwise supervised loss)](https://arxiv.org/abs/2303.15438)\n",
    "- Learn joint distribution : It is expensive to create GANs with different combinations of facial characters P(blond, female, smiling, with glasses), P(brown, male, smiling, no glasses) etc…The curse of dimensionality makes the number of GANs to grow exponentially. Instead, we can learn individual data distribution and combine them to form different distributions. i.e. different attribute combinations.\n",
    "\n",
    "#### Optimization \n",
    "- Professor Forcing involved training an auxiliary discriminator to distinguish between free-running and teacher-forced hidden states, thus encouraging the network’s training and sampling dynamics to converge [2].\n",
    "-  Actor-critic methods [13] have also been proposed, introducing a critic conditioned on target outputs, trained to estimate next-token value functions that guide the actor’s free-running predictions [3]. However, while the motivation for these methods is similar to ours in accounting for stepwise transition dynamics, they are inherently deterministic, and do not accommodate explicitly sampling from a learned distribution—central to our goal of synthetic data generation.\n",
    "\n",
    "### Question \n",
    "- What are the supervised and unsupervised loss?   \n",
    "    - Combinaison de la perte adversaire non supervisée et de la perte supervisée : TimeGAN utilise deux types de pertes lors de l'entraînement. La première est la perte adversaire non supervisée, qui est similaire à celle utilisée dans DCGAN et d'autres GANs. Cette perte encourage le générateur à produire des données qui sont indiscernables de la série temporelle réelle pour le discriminateur. La seconde est une perte supervisée qui compare les séquences générées à la série temporelle réelle. Cette perte encourage le générateur à reproduire les transitions spécifiques d'un point à l'autre dans la série temporelle réelle.\n",
    "- The AR aspect: \n",
    "    - Incorporation de la nature autorégressive des séries temporelles : Les séries temporelles sont souvent autorégressives, ce qui signifie que chaque point de données dépend des points de données précédents. TimeGAN tient compte de cette caractéristique en formant le générateur pour produire non seulement des points de données individuels qui ressemblent à ceux de la série temporelle réelle, mais aussi des séquences de points de données qui maintiennent les mêmes dépendances temporelles.\n",
    "- What part of the architecture is actually learning the temporal dynamics of the data? \n",
    "    - TimeGAN explicitly incorporates the autoregressive nature of time series by combining the unsupervised adversarial loss on both real and synthetic sequences familiar from the DCGAN example with a stepwise supervised loss with respect to the original data. The goal is to reward the model for learning the distribution over transitions from one point in time to the next present in the historical data.\n",
    "    - Answer: 4.1 Embedding and Recovery Functions | The embedding and recovery functions provide mappings between feature and latent space, allowing the adversarial network to learn the underlying temporal dynamics of the data via lower-dimensional representations. \n",
    "- Why does the Generator output in the latent space? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Paper](https://papers.nips.cc/paper/2019/file/c9efe5f26cd17ba6216bbe2a7d26d490-Paper.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Theory](https://papers.nips.cc/paper/2019/file/c9efe5f26cd17ba6216bbe2a7d26d490-Paper.pdf) \n",
    "-  A model is not only tasked with capturing the distributions of features within each time point, it should also capture the potentially complex dynamics of those variables across time\n",
    "- conditional distribution p(xt|x1:t−1) of temporal transitions as well\n",
    "- Autoregressive models explicitly factor the distribution of sequences into a product of conditionals Qt p(xt|x1:t−1). However, while useful in the context of forecasting, this approach is fundamentally deterministic, and is not truly generative in the sense that new sequences can be randomly sampled from them without external conditioning.\n",
    "- Normal GAN on sequential data\n",
    "    - On the other hand, a separate line of work has focused on directly applying the generative adversarial network (GAN) framework to sequential data, primarily by instantiating recurrent networks for the roles of generator and discriminator [4, 5, 6]. While straightforward, the adversarial objective seeks to model p(x1:T ) directly, without leveraging the autoregressive prior. Importantly, simply summing\n",
    "    the standard GAN loss over sequences of vectors may not be sufficient to ensure that the dynamics of the network efficiently captures stepwise dependencies present in the training data\n",
    "\n",
    "- Contribution (new feature): First, in addition to the unsupervised adversarial loss on both real and synthetic sequences, we introduce a stepwise supervised loss using the original data as supervision, thereby explicitly encouraging the model to capture the stepwise conditional distributions in the data.\n",
    "\n",
    "- Second, we introduce an embedding network to provide a reversible mapping between features and latent representations, thereby reducing the high-dimensionality of the adversarial learning space\n",
    "\n",
    "- Supervised loss: Importantly, the supervised loss is minimized by jointly training both the embedding and generator networks, such that the latent space not only serves to promote parameter efficiency—it is specifically conditioned to facilitate the generator in learning temporal relationships.\n",
    "\n",
    "- Our approach is the first to combine the flexibility of the unsupervised GAN framework with the control afforded by supervised training in autoregressive models.\n",
    "\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Litterature review \n",
    "1. Recurrent Conditional GAN (RCGAN) [5] took a similar approach, introducing minor architectural differences such as dropping the dependence on the previous output while conditioning on additional input [14]. A multitude of applied studies have since utilized these frameworks to generate synthetic sequences in such diverse domains as text [15], finance [16], biosignals [17], sensor [18] and smart grid data [19], as well as renewable scenarios [20]. Recent work [6] has proposed conditioning on time stamp information to 2 handle irregularly sampling. However, unlike our proposed technique, these approaches rely only on the binary adversarial feedback for learning, which by itself may not be sufficient to guarantee specifically that the network efficiently captures the temporal dynamics in the training data.\n",
    "2. However, unlike our proposed technique, these approaches rely only on the binary adversarial feedback for learning, which by itself may not be sufficient to guarantee specifically that the network efficiently captures the temporal dynamics in the training data.\n",
    "3.  By contrast, our proposed method generalizes to arbitrary time-series data, incorporates stochasticity at each time step, as well as employing an embedding network to identify a lower-dimensional space for the generative model to learn the stepwise distributions and latent dynamics of the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem formulation\n",
    "Consider the general data setting where each instance consists of two elements: static features (that do not change over time, e.g. gender), and temporal features (that occur over time, e.g. vital signs). Let S be a vector space of static features, X of temporal features, and let S ∈ S, X ∈ X be random vectors that can be instantiated with specific values denoted s and x. We consider tuples of the form (S, X1:T ) with some joint distribution p. \n",
    "\n",
    "Our goal is to use training data D to learn a density pˆ(S, X1:T ) that best approximates p(S, X1:T ). This is a high-level objective, and—depending on the lengths, dimensionality, and distribution of the data—may be difficult to optimize in the standard GAN framework. Therefore we additionally make use of the autoregressive decomposition of the joint p(S, X1:T ) = p(S) Qt p(Xt|S, X1:t−1) to focus specifically on the conditionals, yielding the complementary—and simpler—objective of learning a density p(Xt|S, X1:t−1) that best approximates p(Xt|S, X1:t−1) at any time t.\n",
    "\n",
    "#### Two objectives\n",
    "Importantly, this breaks down the sequence-level objective (matching the joint\n",
    "distribution) into a series of stepwise objectives (matching the conditionals). \n",
    "\n",
    "The first is global, min pˆD\u0010 p(S, X1:T )||pˆ(S, X1:T )) (1) \n",
    "\n",
    "where D is some appropriate measure of distance between distributions. \n",
    "\n",
    "The second is local, min pˆD\u0010 p(Xt|S, X1:t−1)||pˆ(Xt|S, X1:t−1) \u0011 (2) \n",
    "\n",
    "for any t. Under an ideal discriminator in the GAN framework, the former takes the form of the Jensen-Shannon divergence. Using the original data for supervision via   maximum-likelihood (ML) training, the latter takes the form of the Kullback-Leibler divergence. Note that minimizing the former relies on the presence of a perfect adversary (which we may not have access to), while minimizing the latter only depends on the presence of ground-truth sequences (which we do have access to). Our target, then, will be a combination of the GAN objective (proportional to Expression 1) and the ML objective (proportional to Expression 2). As we shall see, this naturally yields a training procedure\n",
    "that involves the simple addition of a supervised loss to guide adversarial learning.\n",
    "\n",
    "### Components \n",
    "\n",
    "#### 1. Embedding & Recovery functions\n",
    "- The embedding and recovery functions provide mappings between feature and latent space, allowing the adversarial network to learn the underlying temporal dynamics of the data via lower-dimensional representations. \n",
    "- Note that the embedding and recovery functions can be parameterized by any architecture of choice, with the only stipulation being that they be autoregressive and obey causal ordering (i.e. output(s) at each step can only depend on preceding information). For example, it is just as possible to implement the former with temporal convolutions [31], or the latter via an attention-based decoder [32].\n",
    "\n",
    "#### 2. Sequence Generator and Discriminator\n",
    "- Instead of producing synthetic output directly in feature space, the generator first outputs into the embedding space.\n",
    "- Random vector zS can be sampled from a distribution of choice, and zt follows a stochastic process; here we use the Gaussian distribution and Wiener process\n",
    "- Finally, the discriminator also operates from the embedding space.\n",
    "- Discriminator Architecture: Similarly, there are no restrictions on architecture beyond the generator being autoregressive; here we use a standard recurrent formulation for ease of exposition.\n",
    "\n",
    "#### 3. Jointly Learning to Encode, Generate, and Iterate\n",
    "- First, purely as a reversible mapping between feature and latent spaces, the embedding and recovery functions should enable accurate reconstructions ˜s, x˜1:T of the original data s, x1:T from their latent representations hS , h1:T . Therefore our first objective function is the reconstruction loss \n",
    "- In TimeGAN, the generator is exposed to two types of inputs during training. First, in pure openloop mode, the generator—which is autoregressive—receives synthetic embeddings hˆS , hˆ1:t−1 (i.e. its own previous outputs) in order to generate the next synthetic vector hˆt. Gradients are then computed on the unsupervised loss. This is as one would  xpect—that is, to allow maximizing (for the discriminator) or minimizing (for the generator) the likelihood of providing correct classifications yˆS , yˆ1:T for both the training data hS , h1:T as well as for synthetic output hˆS , hˆ1:T from the generator,\n",
    "- Relying solely on the discriminator’s binary adversarial feedback may not be sufficient incentive for the generator to capture the stepwise conditional distributions in the data. To achieve this more efficiently, we introduce an additional loss to further discipline learning. In an alternating fashion, we also train in closed-loop mode, where the generator receives sequences of embeddings of actual data h1:t−1 (i.e. computed by the embedding network) to generate the next latent vector. Gradients can now be computed on a loss that captures the discrepancy between distributions p(Ht|HS , H1:t−1) and pˆ(Ht|HS , H1:t−1). \n",
    "- Important: While LU pushes the generator to create realistic sequences (evaluated by an imperfect adversary), LS further ensures that it produces similar stepwise transitions (evaluated by ground-truth targets).\n",
    "\n",
    "### Optimization\n",
    "- θe, θr, θg, θd respectively denote the parameters of the embedding, recovery, generator, and discriminator networks\n",
    "- Importantly, LS is included such that the embedding process not only serves to reduce the dimensions of the adversarial learning space—it is actively conditioned to facilitate the generator in learning temporal relationships from the data.\n",
    "- generator and discriminator networks are trained adversarially as follows min θg (ηLS + maxθd LU)\n",
    "- The embedding task serves to regularize adversarial learning—which now occurs in a lower-dimensional latent space.\n",
    "\n",
    "#### Key point \n",
    "1. Therefore we additionally make use of the autoregressive decomposition of the joint p(S, X1:T ) = p(S)Qt p(Xt|S, X1:t−1) to focus specifically on the conditionals, yielding the complementary—and simpler—objective of learning a density pˆ(Xt|S, X1:t−1) that best approximates p(Xt|S, X1:t−1) at any time t\n",
    "\n",
    "2. Look at expression (1) & (2) of two objectives: As we shall see, this naturally yields a training procedure that involves the simple addition of a supervised loss to guide adversarial learning.\n",
    "\n",
    "3. (Doesn't only receives point at one time point) | In TimeGAN, the generator is exposed to two types of inputs during training. First, in pure openloop mode, the generator—which is autoregressive—receives synthetic embeddings hˆS , hˆ1:t−1 (i.e. its own previous outputs) in order to generate the next synthetic vector hˆt.\n",
    "\n",
    "4. Relying solely on the discriminator’s binary adversarial feedback may not be sufficient incentive for the generator to capture the stepwise conditional distributions in the data. To achieve this more efficiently, we introduce an additional loss to further discipline learning. In an alternating fashion, we also train in closed-loop mode, where the generator receives sequences of embeddings of actual data h1:t−1 (i.e. computed by the embedding network) to generate the next latent vector. Gradients can now be computed on a loss that captures the discrepancy between distributions p(Ht|HS , H1:t−1) and pˆ(Ht|HS , H1:t−1). \n",
    "\n",
    "5. Putting it together: Applying maximum likelihood yields the familiar supervised loss,where gX (hS , ht−1, zt) approximates Ezt∼N [ˆp(Ht|HS , H1:t−1, zt)] with one sample zt—as is standard in stochastic gradient descent. In sum, at any step in a training sequence, we assess the difference between the actual next-step latent vector (from the embedding function) and synthetic next-step latent vector (from the generator—conditioned on the actual historical sequence of latents). While LU pushes the generator to create realistic sequences (evaluated by an imperfect adversary), LS further ensures that it produces similar stepwise transitions (evaluated by ground-truth targets).\n",
    "\n",
    "\n",
    "#### Questions\n",
    "1. Understanding the two objectives \n",
    "2. Why does the discriminator work in the embedding space?\n",
    "    - Réduction de la dimensionnalité : Les espaces d'incorporation ont généralement une dimensionnalité beaucoup plus faible que les données d'entrée d'origine. Cela peut rendre le travail du discriminateur plus facile, car il a moins de dimensions à considérer.\n",
    "    - Extraction de caractéristiques : Les espaces d'incorporation sont généralement conçus pour capturer les caractéristiques importantes des données. Par conséquent, en travaillant dans l'espace d'incorporation, le discriminateur peut se concentrer sur ces caractéristiques importantes et ignorer le bruit et les détails inutiles\n",
    "    - Compatibilité avec le générateur : Si le générateur produit des données dans l'espace d'incorporation (comme c'est souvent le cas dans les GANs), alors il est logique de faire fonctionner le discriminateur dans le même espace.\n",
    "    - Amélioration de la performance : Dans de nombreux cas, faire fonctionner le discriminateur dans l'espace d'incorporation peut améliorer la performance du GAN. Par exemple, cela peut aider à stabiliser l'entraînement et à produire des échantillons générés de meilleure qualité."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps \n",
    "1. Selecting and preparing real and random time series inputs\n",
    "2. Creating the key TimeGAN model components\n",
    "3. Defining the various loss functions and train steps used during the three training phases\n",
    "4. Running the training loops and logging the results\n",
    "5. Generating synthetic time series and evaluating the results\n",
    "\n",
    "[See notebook](https://github.com/stefan-jansen/machine-learning-for-trading/blob/main/21_gans_for_synthetic_time_series/02_TimeGAN_TF2.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import this apple stock price dataset and filter from 2015 to 2021\n",
    "# use this path: /Users/philippebeliveau/Desktop/Notebook_Jupyter_R/Synthetic_Data/Dataset/AAPL.csv\n",
    "import pandas as pd\n",
    "from data_loading import real_data_loading\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "pd.set_option('display.max_columns', None)  \n",
    "df = pd.read_csv('/Users/philippebeliveau/Desktop/Notebook/GAN/Dataset/stock_data.csv')\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "df = df[(df['Date'] >= '2015-01-01') & (df['Date'] <= '2021-01-01')]\n",
    "df.head()\n",
    "\n",
    "# Only keep the columns 'Date' and 'Close'\n",
    "df = df[['Date', 'Close']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data loading\n",
    "data_name = 'stock'\n",
    "seq_len = 24\n",
    "n_seq = 6\n",
    "\n",
    "if data_name in ['stock', 'energy']:\n",
    "  ori_data = real_data_loading(data_name, seq_len)\n",
    "print(data_name + ' dataset is ready.')\n",
    "\n",
    "print(np.array(ori_data).shape)\n",
    "\n",
    "data = ori_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the real data into a PyTorch DataLoader \n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset, RandomSampler\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "data_tensor = torch.tensor(data)\n",
    "\n",
    "# Create a TensorDataset from data\n",
    "# dataset = TensorDataset(data_tensor)\n",
    "\n",
    "# Create a RandomSampler to shuffle the data\n",
    "sampler = RandomSampler(dataset)\n",
    "\n",
    "# Create a DataLoader to batch the data and enable iteration\n",
    "real_series = DataLoader(data_tensor, batch_size=batch_size)\n",
    "\n",
    "# Create an iterator from the DataLoader\n",
    "real_series_iter = iter(real_series)\n",
    "\n",
    "# Show the first batch of real data\n",
    "real_batch = next(real_series_iter)\n",
    "\n",
    "# show me the dimension of the real batch\n",
    "print(real_batch.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure my real_series_iter is a tensor with dtype\n",
    "# Check if seq is a tensor\n",
    "is_tensor = isinstance(real_batch, torch.Tensor)\n",
    "\n",
    "print(is_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "# Create synthetic data\n",
    "from torch.utils.data import DataLoader, IterableDataset\n",
    "\n",
    "class RandomDataset(IterableDataset):\n",
    "    def __init__(self, seq_len, n_seq):\n",
    "        self.seq_len = seq_len\n",
    "        self.n_seq = n_seq\n",
    "\n",
    "    def __iter__(self):\n",
    "        while True:\n",
    "            yield torch.from_numpy(np.random.uniform(low=0, high=1, size=(self.seq_len, self.n_seq))).float()\n",
    "\n",
    "# Create an instance of the dataset\n",
    "dataset = RandomDataset(seq_len, n_seq)\n",
    "\n",
    "# Create a DataLoader to batch the data and enable iteration\n",
    "random_series = DataLoader(dataset, batch_size=batch_size)\n",
    "\n",
    "# Create an iterator from the DataLoader\n",
    "random_series_iter = iter(random_series)\n",
    "\n",
    "# Show the first batch of real data\n",
    "random_series_next = next(random_series_iter)\n",
    "\n",
    "# show me the dimension of the real batch\n",
    "print(random_series_next.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_tensor = isinstance(random_series_next, torch.Tensor)\n",
    "\n",
    "print(is_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network parameters\n",
    "\n",
    "hidden_dim = 24\n",
    "num_layers = 3\n",
    "\n",
    "# Set up tensorboard X\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "# Set up a log directory\n",
    "# log_dir = 'logs'\n",
    "\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedder and recovery network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedder(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(Embedder, self).__init__()\n",
    "        \n",
    "        self.lstm1 = nn.LSTM(input_dim, hidden_dim, num_layers=1)\n",
    "\n",
    "    def forward(self, x): # X is the real input sequence\n",
    "        x, _ = self.lstm1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recovery network\n",
    "class Recovery(torch.nn.Module):\n",
    "    def __init__(self, hidden_dim, seq_len):\n",
    "        super(Recovery, self).__init__()\n",
    "        self.lstm1 = nn.LSTM(hidden_dim, seq_len, num_layers=1)\n",
    "\n",
    "    def forward(self, x): # X is the real input sequence\n",
    "        x, _ = self.lstm1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator & discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator network\n",
    "class Generator(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(Generator, self).__init__()\n",
    "        self.lstm1 = nn.LSTM(input_dim, hidden_dim, num_layers = 1)\n",
    "\n",
    "    def forward(self, x): # X is the real input sequence\n",
    "        x, _ = self.lstm1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discriminator network\n",
    "class Discriminator(torch.nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.lstm1 = nn.LSTM(hidden_dim, hidden_dim, num_layers=1)\n",
    "        self.linear = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x): # X is the real input sequence\n",
    "        x, _ = self.lstm1(x)\n",
    "        x = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervisor loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Supervisor(torch.nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(Supervisor, self).__init__()\n",
    "        self.lstm1 = nn.LSTM(hidden_dim, hidden_dim, num_layers=1)\n",
    "\n",
    "    def forward(self, x): # X is the real input sequence\n",
    "        x, _ = self.lstm1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 1: Autoencoder training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "train_steps = 100\n",
    "gamma = 1\n",
    "mse = nn.MSELoss()\n",
    "bce = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "H = Embedder(n_seq, hidden_dim)\n",
    "\n",
    "R = Recovery(hidden_dim, n_seq)\n",
    "\n",
    "autoencoder = nn.Sequential(H, R)\n",
    "autoencoder_optimizer = torch.optim.Adam(list(H.parameters()) + list(R.parameters()), lr=0.001)\n",
    "\n",
    "# Autoencoder training loop\n",
    "for epoch in tqdm(range(100)):\n",
    "    for i in enumerate(real_series):\n",
    "        autoencoder.train()\n",
    "        real_batch = i[1]\n",
    "\n",
    "        real_batch = real_batch.to(H.lstm1.weight_ih_l0.dtype)\n",
    "        embedder_representation = H(real_batch)\n",
    "        recovered_sequence = R(embedder_representation)\n",
    "        \n",
    "        loss = mse(recovered_sequence, real_batch)\n",
    "        \n",
    "        autoencoder_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        autoencoder_optimizer.step()\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch}, Loss {loss.item()}')\n",
    "    # Return the loss to tensorboard\n",
    "    writer.add_scalar('Autoencoder Loss', loss.item(), epoch)\n",
    "    # Return the model\n",
    "    writer.add_graph(autoencoder, real_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "html"
    }
   },
   "outputs": [],
   "source": [
    "# Now I need to train an autoencoder to make sure that the embedder and recovery network are working\n",
    "R = Recovery(hidden_dim, seq_len)\n",
    "H = Embedder(seq_len, hidden_dim)\n",
    "\n",
    "autoencoder = nn.Sequential(H, R)\n",
    "autoencoder_optimizer = torch.optim.Adam(list(H.parameters()) + list(R.parameters()), lr=0.001)\n",
    "\n",
    "# Autoencoder training loop\n",
    "for epoch in range(100):\n",
    "    for real_batch in real_series_iter:\n",
    "        autoencoder.train()\n",
    "        #real_batch = next(real_series_iter)\n",
    "        real_batch = real_batch[0]\n",
    "        real_batch = real_batch.to(H.lstm1.weight_ih_l0.dtype)\n",
    "        embedder_representation = H(real_batch)\n",
    "        recovered_sequence = R(embedder_representation)\n",
    "        loss = mse(recovered_sequence, real_batch)\n",
    "        autoencoder_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        autoencoder_optimizer.step()\n",
    "        \n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch}, Loss {loss.item()}')\n",
    "    # Return the loss to tensorboard\n",
    "    writer.add_scalar('Autoencoder Loss', loss.item(), epoch)\n",
    "    # Return the model\n",
    "    # writer.add_graph(autoencoder, real_batch)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 2: Supervisor training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    def train_supervisor(x):\n",
    "        with tf.GradientTape() as tape:\n",
    "            h = embedder(x)\n",
    "            h_hat_supervised = supervisor(h)\n",
    "            g_loss_s = mse(h[:, 1:, :], h_hat_supervised[:, :-1, :])\n",
    "\n",
    "        var_list = supervisor.trainable_variables\n",
    "        gradients = tape.gradient(g_loss_s, var_list)\n",
    "        supervisor_optimizer.apply_gradients(zip(gradients, var_list))\n",
    "        return g_loss_s\n",
    "\n",
    "*What is the supervisor training doing? \n",
    "\n",
    "What is the flow?\n",
    "- We know that the embedder must receive real sequence \n",
    "- The recovery then receives the output of the embedder and do his reconstruction \n",
    "We then first need to train an autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supervisor \n",
    "S = Supervisor(hidden_dim)\n",
    "# h_hat_supervisor = S(embedder_representation)\n",
    "\n",
    "# Define the optimizer for the Supervisor\n",
    "supervisor_optimizer = torch.optim.Adam(list(S.parameters()), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the supervisor network\n",
    "for epoch in tqdm(range(100)):\n",
    "    for i in enumerate(real_series):\n",
    "        S.train()\n",
    "        real_batch = i[1]\n",
    "        \n",
    "        real_batch = real_batch.to(H.lstm1.weight_ih_l0.dtype)\n",
    "        embedder_representation = H(real_batch)\n",
    "        h_hat_supervisor = S(embedder_representation)\n",
    "\n",
    "        loss = mse(h_hat_supervisor, embedder_representation)\n",
    "\n",
    "        supervisor_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        supervisor_optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch}, Loss {loss.item()}')\n",
    "    # Return the loss to tensorboard\n",
    "    writer.add_scalar('Supervisor Loss', loss.item(), epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 3: Joint training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adversarial - Supervise\n",
    "\n",
    "Here the supervisor is trained to make the generated sequence look more like the real sequence by incentivizing the generator to  make the supervisor representation look real and learn the temporal aspect. The supervisor is trained with the Embedder, where the embedder learns to map in the latent space the temporal aspect of the real sequence. The supervisor learns to minimize the generated latent sequence from the embedder and his \"refined\" sequence to be as close as possible. And so, having learn the temporal aspect of the latent real sequence, the supervisor learns the temporal aspect. Then, he can supervise the generator to make the generated sequence look more like the real sequence by recontruscting the sequence of the generator in his way, then pass it to the discriminator and classify it as real or fake, then optimize.  \n",
    "\n",
    "In short, the supervisor learns from the embedding space of the real sequence to learn the temporal aspect. \n",
    "Then, goes on to supervisor the generator by adversarily training with it and the discriminator, so has to force the generator to generate sequence that the supervisor will aim at encode further and fool the discriminator. This can make the generator learn better because, the supervisor learned previously to minimize the loss between the latent representation of the real sequence and the latent representation of the supervisor of the real sequence itself. Thus, the supervisor learn the temporal dynamics, which in turn can guide the generator to minimize loss by making the generator generating sequence that possess closer temporal dynamics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Architecture adversarial training\n",
    "G = Generator(n_seq, hidden_dim)\n",
    "D = Discriminator(hidden_dim) # Or n_seq?\n",
    "\n",
    "adversarial_supervised = nn.Sequential(G, S, D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adversarial in latent space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discriminator\n",
    "# Y_fake = D(G(random_sequence_tensor_squeezed))\n",
    "\n",
    "adversarial_network = nn.Sequential(G, D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean & Variance Loss\n",
    "\n",
    "From the generator random vector output in the latent space, the Recovery takes it and reconstruct the sequence. \n",
    "\n",
    "The recovery should be good at reconstructing as it was trained to reconstruct latent real sequence back to original sequence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_hat = R(X_embedded)\n",
    "synthetic_data = nn.Sequential(G, R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_generator_moment_loss(y_true, y_pred):\n",
    "    y_true_mean = y_true.mean(dim=0)\n",
    "    y_true_var = y_true.var(dim=0, unbiased=False)\n",
    "    y_pred_mean = y_pred.mean(dim=0)\n",
    "    y_pred_var = y_pred.var(dim=0, unbiased=False)\n",
    "    g_loss_mean = (y_true_mean - y_pred_mean).abs().mean()\n",
    "    g_loss_var = (y_true_var.sqrt() - y_pred_var.sqrt()).abs().mean()\n",
    "    return g_loss_mean + g_loss_var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminator - Real data\n",
    "\n",
    "This is the classic reconstruction loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y_real = D(H(real_batch_tensor))\n",
    "\n",
    "discriminator_real = nn.Sequential(D, H)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training \n",
    "\n",
    "Now we need to train the network. \n",
    "1. Essentialy, the generator as to learn how to generate proper sequence, this through the adversarial supervise training \n",
    "2. The generator also has to learn how to feel the discriminator \n",
    "3. The generator also has to learn how to generate proper synthetic data using the recovery system. So I believe that after doing the first two part of the training, we can focus on the 3rd part, assuming that the autoencoder training between the embedder and Recovery part was well perform.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the optimizer for the Generator\n",
    "generator_optimizer = torch.optim.Adam(list(G.parameters()) + list(S.parameters()), lr=0.001)\n",
    "\n",
    "# Embedding optimizer \n",
    "embedding_optimizer = torch.optim.Adam(list(H.parameters()) + list(R.parameters()), lr=0.001)\n",
    "\n",
    "# Define the optimizer for the Discriminator\n",
    "discriminator_optimizer = torch.optim.Adam(D.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the different components needed to train the generator? \n",
    "\n",
    "- X_embedded = G(random_sequence_tensor_squeezed) # The generator generates a sequence into the embedding space \n",
    "- supervisor_representation = S(X_embedded) # The supervisor takes that generate sequence and maps it to the supervisor space\n",
    "- Y_hat = D(supervisor_representation) # The discriminator takes the supervisor representation and classifies it as real or fake\n",
    "- Y_fake = D(G(random_sequence_tensor_squeezed))\n",
    "\n",
    "We also have to train the discriminator to become good at classifying the real sequence\n",
    "- Y_real = D(H(real_batch_tensor))\n",
    "\n",
    "We also have to learn how to generate sequence (Recovery and Generator) - and this at the same time of training the other network\n",
    "- X_hat = R(X_embedded)\n",
    "\n",
    "Those are the network that would have to be trained\n",
    "- adversarial_supervised = nn.Sequential(G, S, D)\n",
    "- adversarial_network = nn.Sequential(G, D)\n",
    "- discriminator_real = nn.Sequential(D, H)\n",
    "- synthetic_data = nn.Sequential(G, R)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Generator train step\n",
    "Here the generator will jointly learn how to minimize multiple loss and update its weight accordingly. This is where the Paper means: \n",
    "- Jointly Learning to Encode, Generate, and Iterate\n",
    "\n",
    "1. We get the loss of the adversarial supervised network \n",
    "2. The loss of the adversarial network \n",
    "3. Have the Generator loss supervised - You want your supervisor to become good at minimzing the difference in the distribution between the real latent sequence and the supervisor representation\n",
    "4. The loss of the synthetic data generation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "def train_generator(X, Z):\n",
    "    G.train()\n",
    "    S.train()\n",
    "    \n",
    "    y_fake_e = adversarial_network(Z)\n",
    "    generator_loss_unsupervised =  F.binary_cross_entropy_with_logits(y_fake_e, torch.ones_like(y_fake_e))\n",
    "    \n",
    "    y_fake = adversarial_supervised(Z)\n",
    "    generator_loss_unsupervised_e =  F.binary_cross_entropy_with_logits(y_fake, torch.ones_like(y_fake))\n",
    "    \n",
    "    X1 = X.to(H.lstm1.weight_ih_l0.dtype)\n",
    "    \n",
    "    h = H(X1)\n",
    "    h_hat_supervised = S(h)\n",
    "    generator_loss_supervised = mse(h[:, 1:], h_hat_supervised[:, 1:])\n",
    "\n",
    "    x_hat = synthetic_data(Z)\n",
    "    generator_moment_loss = get_generator_moment_loss(X, x_hat)\n",
    "    \n",
    "    # Loss for the generator\n",
    "    generator_loss = (generator_loss_unsupervised +\n",
    "                      generator_loss_unsupervised_e +\n",
    "                      100 * torch.sqrt(generator_loss_supervised) +\n",
    "                      100 * generator_moment_loss)\n",
    "    generator_loss.backward()\n",
    "    generator_optimizer.step()\n",
    "    \n",
    "    generator_optimizer.zero_grad()\n",
    "\n",
    "    return generator_loss_unsupervised, generator_loss_supervised, generator_moment_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Embedder train step\n",
    "The embedder is trained jointly with the supervisor, by minimizing two loss function. \n",
    "1. The loss of the renconstruction of the supervisor from the latent representation of the real sequence (Called Generator loss supervised)\n",
    "2. The loss of the Recovey (General reconstruction loss of the autoencoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_embedder(x):\n",
    "    # Ensure the model is in training mode\n",
    "    H.train()\n",
    "    R.train()\n",
    "\n",
    "    # Forward pass\n",
    "    x = x.to(H.lstm1.weight_ih_l0.dtype)\n",
    "    h = H(x)\n",
    "    h_hat_supervised = S(h)\n",
    "    generator_loss_supervised = F.mse_loss(h[:, 1:], h_hat_supervised[:, 1:])\n",
    "\n",
    "    x_tilde = autoencoder(x)\n",
    "    embedding_loss_t0 = F.mse_loss(x, x_tilde)\n",
    "    e_loss = 10 * torch.sqrt(embedding_loss_t0) + 0.1 * generator_loss_supervised\n",
    "\n",
    "    # Backward pass and optimization\n",
    "    e_loss.backward()\n",
    "    embedding_optimizer.step()\n",
    "\n",
    "    # Zero the gradients\n",
    "    embedding_optimizer.zero_grad()\n",
    "\n",
    "    return torch.sqrt(embedding_loss_t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Discriminator train step\n",
    "1. There is the real discriminator loss (y_real)  \n",
    "2. The adversarial supervised loss (y_supervised)\n",
    "3. The adversarial network loss (y_fake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_discriminator_loss(x, z):\n",
    "    # Ensure the model is in training mode\n",
    "    D.train()\n",
    "    # Convert the batch to a tensor and ensure it is the correct type\n",
    "    #z = z.float()\n",
    "\n",
    "    z = z.squeeze(-1) \n",
    "    x = x.squeeze(-1)   \n",
    "\n",
    "    # Forward pass\n",
    "    y_real = D(x)\n",
    "    discriminator_loss_real = F.binary_cross_entropy_with_logits(y_real, torch.ones_like(y_real))\n",
    "\n",
    "    y_fake = adversarial_supervised(z)\n",
    "    discriminator_loss_fake = F.binary_cross_entropy_with_logits(y_fake, torch.zeros_like(y_fake))\n",
    "\n",
    "    y_fake_e = adversarial_network(z)\n",
    "    discriminator_loss_fake_e = F.binary_cross_entropy_with_logits(y_fake_e, torch.zeros_like(y_fake_e))\n",
    "\n",
    "    return discriminator_loss_real + discriminator_loss_fake + gamma * discriminator_loss_fake_e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_discriminator(x, z):\n",
    "    # Ensure the model is in training mode\n",
    "    D.train()\n",
    "\n",
    "    # Forward pass\n",
    "    discriminator_loss = get_discriminator_loss(x, z)\n",
    "\n",
    "    # Backward pass and optimization\n",
    "    discriminator_loss.backward()\n",
    "    discriminator_optimizer.step()\n",
    "\n",
    "    # Zero the gradients\n",
    "    discriminator_optimizer.zero_grad()\n",
    "\n",
    "    return discriminator_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "step_g_loss_u = step_g_loss_s = step_g_loss_v = step_e_loss_t0 = step_d_loss = 0\n",
    "writer = SummaryWriter()\n",
    "\n",
    "train_steps = 100\n",
    "\n",
    "for step in range(train_steps):\n",
    "    for i, j in zip(real_series, random_series):\n",
    "        # Train generator (twice as often as discriminator)\n",
    "        for kk in range(2):\n",
    "            X_, Z_ = i[1], j[1]\n",
    "            \n",
    "            # Train generator\n",
    "            step_g_loss_u, step_g_loss_s, step_g_loss_v = train_generator(X_, Z_)\n",
    "            # Train embedder\n",
    "            step_e_loss_t0 = train_embedder(X_)\n",
    "\n",
    "        X_, Z_ = i[1], j[1]\n",
    "        step_d_loss = get_discriminator_loss(X_, Z_)\n",
    "        if step_d_loss > 0.15:\n",
    "            step_d_loss = train_discriminator(X_, Z_)\n",
    "        \n",
    "        if step % 10 == 0:\n",
    "            print(f'{step:6,.0f} | d_loss: {step_d_loss:6.4f} | g_loss_u: {step_g_loss_u:6.4f} | '\n",
    "                f'g_loss_s: {step_g_loss_s:6.4f} | g_loss_v: {step_g_loss_v:6.4f} | e_loss_t0: {step_e_loss_t0:6.4f}')\n",
    "\n",
    "        writer.add_scalar('G Loss S', step_g_loss_s, step)\n",
    "        writer.add_scalar('G Loss U', step_g_loss_u, step)\n",
    "        writer.add_scalar('G Loss V', step_g_loss_v, step)\n",
    "        writer.add_scalar('E Loss T0', step_e_loss_t0, step)\n",
    "        writer.add_scalar('D Loss', step_d_loss, step)\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the Syntetic Data generator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What log_dir should I put?\n",
    "log_dir = '/Users/philippebeliveau/Desktop/Notebook/GAN/Generated_Data:Synthetizer/V1'\n",
    "synthetic_data.save(log_dir / 'synthetic_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = synthetic_data(Z_)\n",
    "d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_data = []\n",
    "for i in range(int(n_windows / batch_size)):\n",
    "    random_series_iter = iter(random_series)\n",
    "    Z_ = next(random_series_iter)\n",
    "    Z_ = torch.Tensor(Z_[0])\n",
    "    Z_ = Z_.squeeze(-1)\n",
    "    d = synthetic_data(Z_)\n",
    "    generated_data.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Convert each tensor to a numpy array and stack them\n",
    "generated_data_array = np.vstack([tensor.detach().numpy() for tensor in generated_data])\n",
    "# generated_data = np.array(np.vstack(generated_data))\n",
    "generated_data_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generated_data_tensor = torch.stack(generated_data)\n",
    "# np.save(log_dir + 'generated_dataV1.csv', generated_data_tensor.detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rescale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Convert each tensor to a numpy array and stack them\n",
    "# generated_data_array = np.stack([tensor.detach().numpy() for tensor in generated_data])\n",
    "\n",
    "# Now you can reshape and transform the data\n",
    "generated_data = (scaler.inverse_transform(generated_data_array.reshape(-1, n_seq))\n",
    "                  .reshape(-1, seq_len, n_seq))\n",
    "\n",
    "flattened_data = generated_data.ravel()\n",
    "flattened_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Persist data\n",
    "# with pd.HDFStore(hdf_store) as store:\n",
    "#     store.put('data/synthetic', pd.DataFrame(generated_data.reshape(-1, n_seq)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "[See notebook](https://github.com/stefan-jansen/machine-learning-for-trading/blob/main/21_gans_for_synthetic_time_series/03_evaluating_synthetic_data.ipynb)\n",
    "\n",
    "Evaluating the quality of synthetic time-series data\n",
    "The TimeGAN authors assess the quality of the generated data with respect to three practical criteria:\n",
    "\n",
    "Diversity: the distribution of the synthetic samples should roughly match that of the real data\n",
    "Fidelity: the sample series should be indistinguishable from the real data, and\n",
    "Usefulness: the synthetic data should be as useful as their real counterparts for solving a predictive task\n",
    "The authors apply three methods to evaluate whether the synthetic data actually exhibits these characteristics:\n",
    "\n",
    "Visualization: for a qualitative diversity assessment of diversity, we use dimensionality reduction (principal components analysis (PCA) and t-SNE, see Chapter 13) to visually inspect how closely the distribution of the synthetic samples resembles that of the original data\n",
    "Discriminative Score: for a quantitative assessment of fidelity, the test error of a time-series classifier such as a 2-layer LSTM (see Chapter 18) let’s us evaluate whether real and synthetic time series can be differentiated or are, in fact, indistinguishable.\n",
    "Predictive Score: for a quantitative measure of usefulness, we can compare the test errors of a sequence prediction model trained on, alternatively, real or synthetic data to predict the next time step for the real data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot sample time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(df['Date'][0:1440], df['Close'][0:1440], label='Real')\n",
    "plt.plot(df['Date'][0:1440], flattened_data, label='Synthetic')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Close Price')\n",
    "plt.title('Apple Stock Price')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_flower",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
