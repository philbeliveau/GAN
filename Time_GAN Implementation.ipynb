{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TIME GAN implementation \n",
    "\n",
    "## Idea & Challenge \n",
    "- A generative model for time-series data should also learn the temporal dynamics that shapes how one sequence of observations follows another \n",
    "- The model learns a time-series embedding space while optimizing both supervised and adversarial objectives that encourage it to adhere to the dynamics observed while sampling from historical data during training.\n",
    "- A successful generative model for time-series data needs to capture both the cross-sectional distribution of features at each point in time and the longitudinal relationships among these features over time. Expressed in the image context we just discussed, the model needs to learn not only what a realistic image looks like, but also how one image evolves from the next as in a video.\n",
    "\n",
    "#### ! What differentiate it from \"RNN\" Gan \n",
    "TimeGAN explicitly incorporates the autoregressive nature of time series by combining the unsupervised adversarial loss on both real and synthetic sequences familiar from the DCGAN example with a stepwise supervised loss with respect to the original data. The goal is to reward the model for learning the distribution over transitions from one point in time to the next present in the historical data\n",
    "\n",
    "## Ressources\n",
    "- [Kaggle Notebook](https://www.kaggle.com/code/faaizhashmi/generating-synthetic-data-apple-stock-using-gan) \n",
    "- [Medium article ](https://towardsdatascience.com/synthetic-time-series-data-a-gan-approach-869a984f2239)\n",
    "- [Paper](https://papers.nips.cc/paper/2019/file/c9efe5f26cd17ba6216bbe2a7d26d490-Paper.pdf)\n",
    "- [Generative Adversarial Nets for Synthetic Time Series Data](https://github.com/stefan-jansen/machine-learning-for-trading/blob/main/21_gans_for_synthetic_time_series/README.md)\n",
    "\n",
    "## Architecture \n",
    "- TimeGAN architecture introduces the concept of supervised loss —the model is encouraged to capture time conditional distribution within the data by using the original data as a supervision. Also, we can observe the introduction of an embedding network that is responsible to reduce the adversarial learning space dimensionality.\n",
    "\n",
    "- As mentioned above, TimeGAN is a framework to synthesize sequential data compose by 4 networks, that play distinct roles in the process of modelling the data: the expected generator and discriminator, but also, by a recovery and embedder models.\n",
    "\n",
    "### Components (Class) you need to define \n",
    "[See this ](https://github.com/stefan-jansen/machine-learning-for-trading/blob/main/21_gans_for_synthetic_time_series/README.md#the-four-components-of-the-timegan-architecture)\n",
    "1. Supervisor:\n",
    "2. Generator: \n",
    "3. Discriminator: \n",
    "4. Recovery (AE): \n",
    "5. Embedder (AE): \n",
    "\n",
    "You can define your own network (LSTM, GRU, RNN, Transformer) \n",
    "\n",
    "### Loss function\n",
    "1. The reconstruction loss, which refers to the auto-encoder (embedder & recovery), that in a nutshell compares how well was the reconstruction of the encoded data when compared to the original one.\n",
    "2. The supervised loss that, in a nutshell, is responsible to capture how well the generator approximates the next time step in the latent space.\n",
    "3. The unsupervised loss, this one it’s already familiar to us, a it reflects the relation between the generator and discriminator networks (min-max game). \n",
    "\n",
    "### Training phases\n",
    "1. Training the autoencoder on the provided sequential data for optimal reconstruction\n",
    "2. Training the supervisor using the real sequence data to capture the temporal behavior of the historical information, and finally,\n",
    "3. The combined training of four components while minimizing all the three loss functions mentioned previously.\n",
    "\n",
    "### Concept \n",
    "- Embedding network \n",
    "- Time conditional distribution \n",
    "- adversarial learning space\n",
    "- Stepwise dependency (and stepwise supervised loss)\n",
    "- Learn joint distribution : It is expensive to create GANs with different combinations of facial characters P(blond, female, smiling, with glasses), P(brown, male, smiling, no glasses) etc…The curse of dimensionality makes the number of GANs to grow exponentially. Instead, we can learn individual data distribution and combine them to form different distributions. i.e. different attribute combinations.\n",
    "\n",
    "#### Optimization \n",
    "- Professor Forcing involved training an auxiliary discriminator to distinguish between free-running and teacher-forced hidden states, thus encouraging the network’s training and sampling dynamics to converge [2].\n",
    "-  Actor-critic methods [13] have also been proposed, introducing a critic conditioned on target outputs, trained to estimate next-token value functions that guide the actor’s free-running predictions [3]. However, while the motivation for these methods is similar to ours in accounting for stepwise transition dynamics, they are inherently deterministic, and do not accommodate explicitly sampling from a learned distribution—central to our goal of synthetic data generation.\n",
    "\n",
    "### Question \n",
    "- What are the supervised and unsupervised loss?   \n",
    "    - Combinaison de la perte adversaire non supervisée et de la perte supervisée : TimeGAN utilise deux types de pertes lors de l'entraînement. La première est la perte adversaire non supervisée, qui est similaire à celle utilisée dans DCGAN et d'autres GANs. Cette perte encourage le générateur à produire des données qui sont indiscernables de la série temporelle réelle pour le discriminateur. La seconde est une perte supervisée qui compare les séquences générées à la série temporelle réelle. Cette perte encourage le générateur à reproduire les transitions spécifiques d'un point à l'autre dans la série temporelle réelle.\n",
    "- The AR aspect: \n",
    "    - Incorporation de la nature autorégressive des séries temporelles : Les séries temporelles sont souvent autorégressives, ce qui signifie que chaque point de données dépend des points de données précédents. TimeGAN tient compte de cette caractéristique en formant le générateur pour produire non seulement des points de données individuels qui ressemblent à ceux de la série temporelle réelle, mais aussi des séquences de points de données qui maintiennent les mêmes dépendances temporelles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Theory](https://papers.nips.cc/paper/2019/file/c9efe5f26cd17ba6216bbe2a7d26d490-Paper.pdf) \n",
    "-  A model is not only tasked with capturing the distributions of features within each time point, it should also capture the potentially complex dynamics of those variables across time\n",
    "- conditional distribution p(xt|x1:t−1) of temporal transitions as well\n",
    "- Autoregressive models explicitly factor the distribution of sequences into a product of conditionals Qt p(xt|x1:t−1). However, while useful in the context of forecasting, this approach is fundamentally deterministic, and is not truly generative in the sense that new sequences can be randomly sampled from them without external conditioning.\n",
    "- Normal GAN on sequential data\n",
    "    - On the other hand, a separate line of work has focused on directly applying the generative adversarial network (GAN) framework to sequential data, primarily by instantiating recurrent networks for the roles of generator and discriminator [4, 5, 6]. While straightforward, the adversarial objective seeks to model p(x1:T ) directly, without leveraging the autoregressive prior. Importantly, simply summing\n",
    "    the standard GAN loss over sequences of vectors may not be sufficient to ensure that the dynamics of the network efficiently captures stepwise dependencies present in the training data\n",
    "\n",
    "- Contribution (new feature): First, in addition to the unsupervised adversarial loss on both real and synthetic sequences, we introduce a stepwise supervised loss using the original data as supervision, thereby explicitly encouraging the model to capture the stepwise conditional distributions in the data.\n",
    "\n",
    "- Second, we introduce an embedding network to provide a reversible mapping between features and latent representations, thereby reducing the high-dimensionality of the adversarial learning space\n",
    "\n",
    "- Supervised loss: Importantly, the supervised loss is minimized by jointly training both the embedding and generator networks, such that the latent space not only serves to promote parameter efficiency—it is specifically conditioned to facilitate the generator in learning temporal relationships.\n",
    "\n",
    "- Our approach is the first to combine the flexibility of the unsupervised GAN framework with the control afforded by supervised training in autoregressive models.\n",
    "\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Litterature review \n",
    "1. Recurrent Conditional GAN (RCGAN) [5] took a similar approach, introducing minor architectural differences such as dropping the dependence on the previous output while conditioning on additional input [14]. A multitude of applied studies have since utilized these frameworks to generate synthetic sequences in such diverse domains as text [15], finance [16], biosignals [17], sensor [18] and smart grid data [19], as well as renewable scenarios [20]. Recent work [6] has proposed conditioning on time stamp information to 2 handle irregularly sampling. However, unlike our proposed technique, these approaches rely only on the binary adversarial feedback for learning, which by itself may not be sufficient to guarantee specifically that the network efficiently captures the temporal dynamics in the training data.\n",
    "2. However, unlike our proposed technique, these approaches rely only on the binary adversarial feedback for learning, which by itself may not be sufficient to guarantee specifically that the network efficiently captures the temporal dynamics in the training data.\n",
    "3.  By contrast, our proposed method generalizes to arbitrary time-series data, incorporates stochasticity at each time step, as well as employing an embedding network to identify a lower-dimensional space for the generative model to learn the stepwise distributions and latent dynamics of the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation\n",
    "\n",
    "## Steps \n",
    "1. Selecting and preparing real and random time series inputs\n",
    "2. Creating the key TimeGAN model components\n",
    "3. Defining the various loss functions and train steps used during the three training phases\n",
    "4. Running the training loops and logging the results\n",
    "5. Generating synthetic time series and evaluating the results\n",
    "\n",
    "[See notebook](https://github.com/stefan-jansen/machine-learning-for-trading/blob/main/21_gans_for_synthetic_time_series/02_TimeGAN_TF2.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "[See notebook](https://github.com/stefan-jansen/machine-learning-for-trading/blob/main/21_gans_for_synthetic_time_series/03_evaluating_synthetic_data.ipynb)\n",
    "\n",
    "Evaluating the quality of synthetic time-series data\n",
    "The TimeGAN authors assess the quality of the generated data with respect to three practical criteria:\n",
    "\n",
    "Diversity: the distribution of the synthetic samples should roughly match that of the real data\n",
    "Fidelity: the sample series should be indistinguishable from the real data, and\n",
    "Usefulness: the synthetic data should be as useful as their real counterparts for solving a predictive task\n",
    "The authors apply three methods to evaluate whether the synthetic data actually exhibits these characteristics:\n",
    "\n",
    "Visualization: for a qualitative diversity assessment of diversity, we use dimensionality reduction (principal components analysis (PCA) and t-SNE, see Chapter 13) to visually inspect how closely the distribution of the synthetic samples resembles that of the original data\n",
    "Discriminative Score: for a quantitative assessment of fidelity, the test error of a time-series classifier such as a 2-layer LSTM (see Chapter 18) let’s us evaluate whether real and synthetic time series can be differentiated or are, in fact, indistinguishable.\n",
    "Predictive Score: for a quantitative measure of usefulness, we can compare the test errors of a sequence prediction model trained on, alternatively, real or synthetic data to predict the next time step for the real data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
